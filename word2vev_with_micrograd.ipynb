{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPARATION\n",
    "\n",
    "- [Read more from here](https://towardsdatascience.com/a-word2vec-implementation-using-numpy-and-python-d256cf0e5f28)\n",
    "- [Theory from here](http://www.claudiobellei.com/2018/01/06/backprop-word2vec/#skipgram)\n",
    "\n",
    "The training data needs to be in the following format. \n",
    "\n",
    "Example:\n",
    "\n",
    "    Window size = 2, Vocab size = 9\n",
    "\n",
    "\n",
    "    We will set the indicies as 1 according to the word_to_index dict i.e natural : 0,  so we set the 0th index as 1 to denote natural\n",
    "\n",
    "    Target word = best    \n",
    "    Context words = (way,to)\n",
    "    Target_word_one_hot_vector = [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    Context_word_one_hot_vector = [0, 1, 1, 0, 0, 0, 0, 0, 0]\n",
    "    \n",
    "    Target word = way    \n",
    "    Context words = (best,to,success)\n",
    "    Target_word_one_hot_vector = [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "    Context_word_one_hot_vector= [1, 0, 1, 1, 0, 0, 0, 0, 0]\n",
    "    \n",
    "\n",
    "\n",
    "Thus we need to take the text \n",
    "\n",
    "1. Encode it into hot encoded vectors\n",
    "2. In this case we are using Skip-gram which build model that, tries to learn the context words for each of the target words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_utils.word_2_vec_dataprep import *\n",
    "\n",
    "\n",
    "def prepare_training_data(text):\n",
    "    word_to_index,index_to_word,corpus,vocab_size,length_of_corpus = generate_dictionary_data(text)\n",
    "    return vocab_size, generate_training_data(corpus, 3, vocab_size= vocab_size, word_to_index=word_to_index,length_of_corpus=length_of_corpus)\n",
    "\n",
    "\n",
    "\n",
    "def sample_training_data(target_word_vec,context_word_vec,index_to_word):\n",
    "    print(f\"Vocab has \" + str(len(target_word_vec)) + \" words\")\n",
    "    print(\"Target Word\")\n",
    "    for idx,val in enumerate(target_word_vec):\n",
    "        if val == 1:\n",
    "            print(f\"{idx} : {index_to_word[idx]}\")\n",
    "\n",
    "    print(\"Context Words\")\n",
    "    for idx,val in enumerate(context_word_vec):\n",
    "        if val == 1:\n",
    "            print(f\"{idx} : {index_to_word[idx]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAKE TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size : 56\n",
      "Vocab size : 44\n"
     ]
    }
   ],
   "source": [
    "text = []\n",
    "# with open('data/jef_archer.txt') as f:\n",
    "#     for line in f:\n",
    "#         text.append(line)\n",
    "        \n",
    "\n",
    "text = \"Abel dies soon after, and bequeathes everything to his daughter Florentyna, except his silver band of authority, which he leaves to his grandson, whom Florentyna and Richard have named Harry Clifton has joined the British Navy and has assumed the identity of Tom Bradshaw after his ship sinks in order to solve some of his problems\".split()\n",
    "\n",
    "vocab_size,training_data = prepare_training_data(text=text)\n",
    "\n",
    "print(f\"Corpus size : {len(text)}\")\n",
    "print(f\"Vocab size : {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING NOW THAT TRAINING DATA IS READY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_utils.micrograd import *\n",
    "\n",
    "neural_network = MLP(vocab_size, [6, vocab_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 0\n",
      "Has 6 neurons\n",
      "Each neuron has 45 inputs\n",
      "\n",
      "Layer: 1\n",
      "Has 44 neurons\n",
      "Each neuron has 7 inputs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "neural_network.represent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1; Error : Value(data=2789.618727530883)\n",
      "Iteration 2; Error : Value(data=626.5161021165817)\n",
      "Iteration 3; Error : Value(data=1148.9076093187746)\n",
      "Iteration 4; Error : Value(data=596.1076855262487)\n",
      "Iteration 5; Error : Value(data=496.68296883171655)\n",
      "Iteration 6; Error : Value(data=274.75579685626974)\n",
      "Iteration 7; Error : Value(data=313.05758375010834)\n",
      "Iteration 8; Error : Value(data=298.0044847762304)\n",
      "Iteration 9; Error : Value(data=304.9200292189766)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# # Put training data in right form \n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "xs = [x for x,_ in training_data]\n",
    "ys = [y for _,y in training_data]\n",
    "\n",
    "# Perform forward propagation for all x values with current neural network\n",
    "# Store predictions in yout\n",
    "\n",
    "for i in range(1,10):\n",
    "    yout = [neural_network(x) for x in xs]\n",
    "    def substract(arr1,arr2):\n",
    "        return sum(np.square(arr1-arr2))\n",
    "\n",
    "    err = sum([ substract(y_pred,y) for y_pred,y in zip(yout,ys)]) \n",
    "\n",
    "    print(f\"Iteration {i}; Error : {err}\")\n",
    "\n",
    "    err.backward()\n",
    "\n",
    "    all_params = neural_network.parameters()\n",
    "\n",
    "    for param in all_params:\n",
    "        param.data =- param.grad * learning_rate/i**2\n",
    "        param.grad = 0.0 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{W_111=-0.5616756035848232 | grad=-0.8953650036662133 | grad_updates=56}\n"
     ]
    }
   ],
   "source": [
    "print(all_params[0].get_node_label())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
