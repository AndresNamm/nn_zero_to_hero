{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPARATION\n",
    "\n",
    "- [Read more from here](https://towardsdatascience.com/a-word2vec-implementation-using-numpy-and-python-d256cf0e5f28)\n",
    "- [Theory from here](http://www.claudiobellei.com/2018/01/06/backprop-word2vec/#skipgram)\n",
    "\n",
    "The training data needs to be in the following format. \n",
    "\n",
    "Example:\n",
    "\n",
    "    Window size = 2, Vocab size = 9\n",
    "\n",
    "\n",
    "    We will set the indicies as 1 according to the word_to_index dict i.e natural : 0,  so we set the 0th index as 1 to denote natural\n",
    "\n",
    "    Target word = best    \n",
    "    Context words = (way,to)\n",
    "    Target_word_one_hot_vector = [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    Context_word_one_hot_vector = [0, 1, 1, 0, 0, 0, 0, 0, 0]\n",
    "    \n",
    "    Target word = way    \n",
    "    Context words = (best,to,success)\n",
    "    Target_word_one_hot_vector = [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "    Context_word_one_hot_vector= [1, 0, 1, 1, 0, 0, 0, 0, 0]\n",
    "    \n",
    "\n",
    "\n",
    "Thus we need to take the text \n",
    "\n",
    "1. Encode it into hot encoded vectors\n",
    "2. In this case we are using Skip-gram which build model that, tries to learn the context words for each of the target words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_utils.word_2_vec_dataprep import *\n",
    "\n",
    "\n",
    "def prepare_training_data(text):\n",
    "    word_to_index,index_to_word,corpus,vocab_size,length_of_corpus = generate_dictionary_data(text)\n",
    "    return vocab_size, generate_training_data(corpus, 3, vocab_size= vocab_size, word_to_index=word_to_index,length_of_corpus=length_of_corpus)\n",
    "\n",
    "\n",
    "\n",
    "def sample_training_data(target_word_vec,context_word_vec,index_to_word):\n",
    "    print(f\"Vocab has \" + str(len(target_word_vec)) + \" words\")\n",
    "    print(\"Target Word\")\n",
    "    for idx,val in enumerate(target_word_vec):\n",
    "        if val == 1:\n",
    "            print(f\"{idx} : {index_to_word[idx]}\")\n",
    "\n",
    "    print(\"Context Words\")\n",
    "    for idx,val in enumerate(context_word_vec):\n",
    "        if val == 1:\n",
    "            print(f\"{idx} : {index_to_word[idx]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAKE TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size : 56\n",
      "Vocab size : 44\n"
     ]
    }
   ],
   "source": [
    "text = []\n",
    "# with open('data/jef_archer.txt') as f:\n",
    "#     for line in f:\n",
    "#         text.append(line)\n",
    "        \n",
    "\n",
    "text = \"Abel dies soon after, and bequeathes everything to his daughter Florentyna, except his silver band of authority, which he leaves to his grandson, whom Florentyna and Richard have named Harry Clifton has joined the British Navy and has assumed the identity of Tom Bradshaw after his ship sinks in order to solve some of his problems\".split()\n",
    "\n",
    "vocab_size,training_data = prepare_training_data(text=text)\n",
    "\n",
    "print(f\"Corpus size : {len(text)}\")\n",
    "print(f\"Vocab size : {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING NOW THAT TRAINING DATA IS READY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 0\n",
      "Has 15 neurons\n",
      "Each neuron has 45 inputs\n",
      "\n",
      "Layer: 1\n",
      "Has 44 neurons\n",
      "Each neuron has 16 inputs\n",
      "\n",
      "Iteration 1; Error : Value(data=1581.3883541645753)\n",
      "Iteration 2; Error : Value(data=2291.4303020378993)\n",
      "Iteration 3; Error : Value(data=2360.5586901053466)\n",
      "Iteration 4; Error : Value(data=1513.7061715870498)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Perform forward propagation for all x values with current neural network\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Store predictions in yout\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 19\u001b[0m     yout \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mneural_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msubstract\u001b[39m(arr1,arr2):\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(np\u001b[38;5;241m.\u001b[39msquare(arr1\u001b[38;5;241m-\u001b[39marr2))\n",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Perform forward propagation for all x values with current neural network\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Store predictions in yout\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 19\u001b[0m     yout \u001b[38;5;241m=\u001b[39m [\u001b[43mneural_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m xs]\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msubstract\u001b[39m(arr1,arr2):\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(np\u001b[38;5;241m.\u001b[39msquare(arr1\u001b[38;5;241m-\u001b[39marr2))\n",
      "File \u001b[0;32m~/repod/nn_zero_to_hero/my_utils/micrograd.py:201\u001b[0m, in \u001b[0;36mMLP.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 201\u001b[0m         x\u001b[38;5;241m=\u001b[39m\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/repod/nn_zero_to_hero/my_utils/micrograd.py:182\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    180\u001b[0m out \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m neuron \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons:\n\u001b[0;32m--> 182\u001b[0m     out\u001b[38;5;241m.\u001b[39mappend(\u001b[43mneuron\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m out\n",
      "File \u001b[0;32m~/repod/nn_zero_to_hero/my_utils/micrograd.py:164\u001b[0m, in \u001b[0;36mNeuron.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m,x): \u001b[38;5;66;03m# Returns scalar value between -1 and 1\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m((xi\u001b[38;5;241m*\u001b[39mwi \u001b[38;5;28;01mfor\u001b[39;00m xi,wi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(x,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw)),\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb); n\u001b[38;5;241m.\u001b[39mlabel \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    165\u001b[0m     tan \u001b[38;5;241m=\u001b[39m n\u001b[38;5;241m.\u001b[39mtanh(); tan\u001b[38;5;241m.\u001b[39mlabel \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tan\n",
      "File \u001b[0;32m~/repod/nn_zero_to_hero/my_utils/micrograd.py:164\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m,x): \u001b[38;5;66;03m# Returns scalar value between -1 and 1\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m((\u001b[43mxi\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mwi\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m xi,wi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(x,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw)),\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb); n\u001b[38;5;241m.\u001b[39mlabel \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    165\u001b[0m     tan \u001b[38;5;241m=\u001b[39m n\u001b[38;5;241m.\u001b[39mtanh(); tan\u001b[38;5;241m.\u001b[39mlabel \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tan\n",
      "File \u001b[0;32m~/repod/nn_zero_to_hero/my_utils/micrograd.py:76\u001b[0m, in \u001b[0;36mValue.__rmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__truediv__\u001b[39m(\u001b[38;5;28mself\u001b[39m,other):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m*\u001b[39m other\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__rmul__\u001b[39m(\u001b[38;5;28mself\u001b[39m,other): \u001b[38;5;66;03m# https://stackoverflow.com/questions/5181320/under-what-circumstances-are-rmul-called\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m*\u001b[39m other\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtanh\u001b[39m(\u001b[38;5;28mself\u001b[39m): \u001b[38;5;66;03m# https://wikimedia.org/api/rest_v1/media/math/render/svg/b8dc4c309a551cafc2ce5c883c924ecd87664b0f\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from my_utils.micrograd import *\n",
    "\n",
    "neural_network = MLP(vocab_size, [15, vocab_size])\n",
    "# # Put training data in right form \n",
    "neural_network.represent()\n",
    "\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "xs = [x for x,_ in training_data]\n",
    "ys = [y for _,y in training_data]\n",
    "\n",
    "# Perform forward propagation for all x values with current neural network\n",
    "# Store predictions in yout\n",
    "\n",
    "for i in range(1,10):\n",
    "    yout = [neural_network(x) for x in xs]\n",
    "    def substract(arr1,arr2):\n",
    "        return sum(np.square(arr1-arr2))\n",
    "\n",
    "    err = sum([ substract(y_pred,y) for y_pred,y in zip(yout,ys)]) \n",
    "\n",
    "    print(f\"Iteration {i}; Error : {err}\")\n",
    "\n",
    "    for param in neural_network.parameters():    \n",
    "        param.grad = 0.0 \n",
    "    \n",
    "    err.backward()\n",
    "\n",
    "    for param in neural_network.parameters():\n",
    "        param.data =- param.grad * learning_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{W_111=-0.5616756035848232 | grad=-0.8953650036662133 | grad_updates=56}\n"
     ]
    }
   ],
   "source": [
    "print(all_params[0].get_node_label())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
