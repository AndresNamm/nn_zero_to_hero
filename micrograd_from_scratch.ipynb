{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTUITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 3*x**2 - 4*x + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.arange(-5,5,0.25)\n",
    "ys = f(xs)\n",
    "\n",
    "plot=plt.plot(xs,ys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DERIVATIVE OF FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=0.0001\n",
    "x=3.0\n",
    "dfdx = ((f(x+h)-f(x))/h)\n",
    "print(dfdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slope(x,h=0.0001):\n",
    "    return ((f(x+h)-f(x))/h)\n",
    "\n",
    "\n",
    "xs = np.arange(-5,5,0.25)\n",
    "ys = slope(xs)\n",
    "\n",
    "plot=plt.plot(xs,ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "\n",
    "    def __init__(self,data, _children=(), _op='',label=''):\n",
    "        self.data:float = data\n",
    "        self.grad:float = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        self._gradient_updates=0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    def get_node_label(self):\n",
    "            \n",
    "            label = f\"{self.label}={self.data}\" if self.label else self.data\n",
    "            return \"{\" + f\"{label} | grad={self.grad}\" + \" | \" + f\"grad_updates={self._gradient_updates}\" + \"}\"\n",
    "    \n",
    "    def __add__(self,other): \n",
    "        other = other if isinstance(other,Value) else Value(other)\n",
    "        out =  Value(self.data + other.data, {self,other}, _op='+')\n",
    "\n",
    "        def _backward(): # This backward function is called from the out variable, aka the result value (parent), however the _backward function assigns gradients to input variables (children) leveraging its own gradient # \n",
    "            self.grad += 1 * out.grad\n",
    "            other.grad += 1 * out.grad\n",
    "            self._gradient_updates+=1\n",
    "            other._gradient_updates+=1\n",
    "        out._backward = _backward\n",
    "    \n",
    "        return out \n",
    "    \n",
    "    def __radd__(self,other):# https://stackoverflow.com/questions/9126766/addition-between-classes-using-radd-method\n",
    "        return self+other\n",
    "\n",
    "    def __mul__(self,other): \n",
    "        other = other if isinstance(other,Value) else Value(other)\n",
    "        out = Value(self.data * other.data, {self,other}, _op='*')\n",
    "\n",
    "        def _backward():\n",
    "            # We are adding to gradient because the value could have effect\n",
    "            # through multiple later stages. \n",
    "            self.grad += other.data * out.grad \n",
    "            other.grad += self.data * out.grad\n",
    "            self._gradient_updates+=1\n",
    "            other._gradient_updates+=1\n",
    "\n",
    "        out._backward = _backward   \n",
    "        return out\n",
    "    \n",
    "    def __pow__(self,other):\n",
    "        assert isinstance(other,(int,float))\n",
    "\n",
    "        out = Value(self.data**other,{self,},_op=f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * self.data ** (other-1) * out.grad\n",
    "            self._gradient_updates+=1\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out   \n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * (-1)\n",
    "\n",
    "    def __sub__(self,other):\n",
    "        return self + (-other)\n",
    "    \n",
    "    def __truediv__(self,other):\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rmul__(self,other): # https://stackoverflow.com/questions/5181320/under-what-circumstances-are-rmul-called\n",
    "        return self * other\n",
    "\n",
    "    def tanh(self): # https://wikimedia.org/api/rest_v1/media/math/render/svg/b8dc4c309a551cafc2ce5c883c924ecd87664b0f\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "        out = Value(t, (self, ), 'tanh')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 - out.data**2) * out.grad\n",
    "            self._gradient_updates+=1\n",
    "\n",
    "        out._backward = _backward     \n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        x=self.data\n",
    "        out = Value(math.exp(x), (self,),'exp')\n",
    "        def _backward():\n",
    "            self.grad += math.exp(x) * out.grad\n",
    "            self._gradient_updates+=1\n",
    "\n",
    "        out._backward=_backward\n",
    "        return out \n",
    "        \n",
    "    \n",
    "    def backward(self): \n",
    "        visited=set()\n",
    "        topo=[]\n",
    "        def build_topo(o):\n",
    "            if o not in visited:\n",
    "                visited.add(o)\n",
    "                for node in o._prev:\n",
    "                    build_topo(node)\n",
    "                topo.append(o)\n",
    "        self.grad=1\n",
    "        build_topo(self)\n",
    "\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "a = Value(2.0,label='a')\n",
    "b = Value(-3.0,label='b')\n",
    "c = Value(10.0,label='c')\n",
    "e=a*b; e.label = 'e'\n",
    "d=e+c; d.label = 'd'\n",
    "f= Value(-2.0,label='f')\n",
    "L = d*f; L.label = 'L'\n",
    "\n",
    "L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "  # builds a set of all nodes and edges in a graph. Runs a DFS from the answer to all the components\n",
    "  # that contributed to it.\n",
    "  nodes, edges = set(), set()\n",
    "  def build(v):\n",
    "    if v not in nodes:\n",
    "      nodes.add(v)\n",
    "      for child in v._prev:\n",
    "        edges.add((child, v))\n",
    "        build(child)\n",
    "  build(root)\n",
    "  return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "  nodes, edges = trace(root)\n",
    "  \n",
    "  for n in nodes:\n",
    "    uid = str(id(n))\n",
    "    # for any value in the graph, create a rectangular ('record') node for it\n",
    "    #dot.node(name = uid, label = \"{ %s | data %.4f |  }\" % ( n.label ,n.data), shape='record')\n",
    "    dot.node(name = uid, label = n.get_node_label(), shape='record')\n",
    "\n",
    "    if n._op:\n",
    "      # if this value is a result of some operation, create an op node for it\n",
    "      dot.node(name = uid + n._op, label = n._op)\n",
    "      # and connect this node to it\n",
    "      dot.edge(uid + n._op, uid)\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    # connect n1 to the op node of n2\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "  return dot\n",
    "\n",
    "\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB** The thing to note about this Value based calculation implementation is that here we are doing every calculation pairwise. \n",
    "This allows us to very easily define partial derivatives for each result of pairwise operation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOING GRADIENT CALCULATIONS IN NN EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_global_weights():\n",
    "    global x1, x2, w1, w2, b, x1w1, x2w2, x1w1x2w2, n, o\n",
    "    x1 = Value(2.0, label='x1')\n",
    "    x2 = Value(0.0, label='x2')\n",
    "    # weights w1,w2\n",
    "    w1 = Value(-3.0, label='w1')\n",
    "    w2 = Value(1.0, label='w2')\n",
    "    # bias of the neuron\n",
    "    b = Value(6.8813735870195432, label='b')\n",
    "    # x1*w1 + x2*w2 + b\n",
    "    x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "    x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "    x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "    n = x1w1x2w2 + b; n.label = 'n'\n",
    "    o = n.tanh(); o.label = 'o'\n",
    "initialize_global_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE - FINDING MANUAL GRADIENTS FOR EACH PARAMETER FOR FUNCTION O()\n",
    "\n",
    "In this example, we could imagine O is some kind of Cost function. \n",
    "\n",
    "- leveraging [chain rule](https://www.notion.so/dataleaper/Chain-Rule-6ffc28788e9a46dc8f770dde793fda39?pvs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.grad = 1.0 # dO/dO = 1\n",
    "# o = tanh(n)\n",
    "# dO/dn = 1.0 - tanh(n)**2 = 1 - o**2 \n",
    "n.grad = (1.0  - o.data**2) # derivative in o.data location\n",
    "# need to leverage chain rule here to find dO/dw1\n",
    "# dO/db = dO/Dn * dN/db \n",
    "# dN/db = 1 -> derivative on b inside a sum is 1 -- https://www.mathsisfun.com/calculus/derivatives-rules.html\n",
    "b.grad = (1) * n.grad \n",
    "# dO/dx1w1x2w2 = dO/dn * dN/dx1w1x2w2\n",
    "# dN/dx1w1x2w2 -> derivative of x1w1x2w2 inside function b + x1w1x2w2 is 1 \n",
    "x1w1x2w2.grad = (1) * n.grad\n",
    "# dO/x1w1 = dO/dx1w1x2w2 * dX1W1X2W2/x1w1 #\n",
    "# dX1W1X2W2/x1w1 -> derivative of x1w1 inside sum is 1\n",
    "x1w1.grad = (1) * x1w1x2w2.grad\n",
    "# dO/x2w2 = dO/dx1w1x2w2*dX1W1X2W2/x2w2#\n",
    "# dX1W1X2W2/x2w2 -> derivative of x2w2 inside sum is 1\n",
    "x2w2.grad = (1) * x1w1x2w2.grad\n",
    "# d0/w1 = dO/x1w1 * dX1W1/dw1  \n",
    "# dX1W1/dw1 = x1\n",
    "w1.grad =  (x1.data) * x1w1.grad \n",
    "# d0/w2 = dO/x2w2 * dX2W2/dw2\n",
    "# dX2W2/dw2 = x2\n",
    "w2.grad = (x2.data) * n.grad \n",
    "# dO/dx1 = dO/x1w1 * dX1W1/dx1\n",
    "# dX1W1/dx1 = w1\n",
    "x1.grad = (w1.data) * x1w1.grad\n",
    "# dO/dx2 = dO/x2w2 * dX2W2/dx2\n",
    "# dX2W2/dx2 = w2\n",
    "x2.grad = (w2.data) * x2w2.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_global_weights()\n",
    "o.grad = 1.0\n",
    "o._backward()\n",
    "draw_dot(o) # Calculates the gradients for the input value. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n._backward()\n",
    "b._backward()\n",
    "x1w1x2w2._backward()\n",
    "x1w1._backward()\n",
    "x2w2._backward()\n",
    "w1._backward()\n",
    "w2._backward()\n",
    "x1._backward()\n",
    "x2._backward()\n",
    "draw_dot(o)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENTING BACKPROPAGATION WITH TOPOLOGICAL SORT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use topological sort to implement backpropagation\n",
    "\n",
    "1. We maintain list of visited nodes as there are many ways in NN we can get from to a certain node. \n",
    "2. We get topological sort with reverse DFS as it guarantees that the parent always is called before its children\n",
    "\n",
    "\n",
    "\n",
    "![](https://matthewmazur.files.wordpress.com/2018/03/neural_network-9.png)\n",
    "\n",
    "\n",
    "\n",
    "I added this picture here to explain why in neural network there is possibility to go to children not many times from root node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUGFIX \n",
    "\n",
    "- Gradients are overwritten if one variable occurs as a dependency to multiple later variables (Like in regulard NN) \n",
    "- https://youtu.be/VMj-3S1tku0?si=iaHDwg_ni1FrIEJd&t=5145\n",
    "- Solution: Use +=  to gradient when backwards function is called. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPENING OUT TANGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_global_weights_v2():\n",
    "    global x1, x2, w1, w2, b, x1w1, x2w2, x1w1x2w2, n, o\n",
    "    x1 = Value(2.0, label='x1')\n",
    "    x2 = Value(0.0, label='x2')\n",
    "    # weights w1,w2\n",
    "    w1 = Value(-3.0, label='w1')\n",
    "    w2 = Value(1.0, label='w2')\n",
    "    # bias of the neuron\n",
    "    b = Value(6.8813735870195432, label='b')\n",
    "    # x1*w1 + x2*w2 + b\n",
    "    x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "    x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "    x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "    n = x1w1x2w2 + b; n.label = 'n'\n",
    "    # ----\n",
    "    # o = n.tanh(); o.label = 'o'\n",
    "    # instead we are going to do \n",
    "    e = (2*n).exp()\n",
    "    o = (e-1)/(e+1)\n",
    "\n",
    "\n",
    "initialize_global_weights_v2()\n",
    "\n",
    "o.backward()\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOING THE SAME THING BUT IN PYTHORCH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x1 = torch.Tensor([2.0]).double()               ; x1.requires_grad=True\n",
    "x2 = torch.Tensor([0.0]).double()               ; x2.requires_grad=True\n",
    "w1 = torch.Tensor([-3.0]).double()              ; w1.requires_grad=True\n",
    "w2 = torch.Tensor([1.0]).double()               ; w2.requires_grad=True\n",
    "b = torch.Tensor([6.8813735870195432]).double() ; b.requires_grad=True\n",
    "n = x1*w1+x2*w2+b\n",
    "o = torch.tanh(n)\n",
    "\n",
    "\n",
    "o.backward()\n",
    "\n",
    "print(o.data.item())\n",
    "\n",
    "print('----')\n",
    "print(f'x1: {x1.grad.item()}')\n",
    "print(f'w1: {w1.grad.item()}')\n",
    "print(f'x2: {x2.grad.item()}')\n",
    "print(f'w2: {w2.grad.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LETS START WITH BUILDING OUT A NEURAL NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self,nin,layer_idx,neuron_idx):\n",
    "        self.w = [Value(random.uniform(-1, 1),label=f'W_{layer_idx}{neuron_idx}{parameter_idx+1}') for parameter_idx in range(nin)]\n",
    "        self.b = Value(random.uniform(-1,1),label=f'B_{layer_idx}{neuron_idx}')\n",
    "    \n",
    "    def __call__(self,x): # Returns scalar value between -1 and 1\n",
    "        n = sum((xi*wi for xi,wi in zip(x,self.w)),self.b); n.label = 'n'\n",
    "        tan = n.tanh(); tan.label = 'tanh'\n",
    "        return tan\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "    \n",
    "class Layer:\n",
    "\n",
    "    def __init__(self, nin, nout, layer_idx): # Input is how many inputs each neuron takes. It's the number of dimensions in\n",
    "        # n-1 layer, output is how many neurons will be generated. \n",
    "        self.neurons = []\n",
    "        for neuron_idx in range(nout):\n",
    "            self.neurons.append(Neuron(nin,layer_idx,neuron_idx+1))\n",
    "\n",
    "    def __call__(self,x):\n",
    "        out = []\n",
    "        for neuron in self.neurons:\n",
    "            out.append(neuron(x))\n",
    "\n",
    "        return out[0] if len(out)==1 else out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [parameter for neuron in self.neurons for parameter in neuron.parameters() ]#\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, nin, layers):\n",
    "        sz = [nin]+layers\n",
    "        self.layers = []\n",
    "        for i in range(1, len(sz)): # generate neurons for each layer. We start from 1 not 0 index because the 0th index describes size of data not neural network layer. It's needed to determine firs layer neuron size.  \n",
    "            # Each neuron in layer sz[i-1] inputs \n",
    "            # Each layer has sz[i] neurons \n",
    "            self.layers.append(Layer(sz[i-1],sz[i],i))\n",
    "\n",
    "\n",
    "    def __call__(self,x):\n",
    "        for layer in self.layers:\n",
    "            x=layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [parameter for layer in self.layers for parameter in layer.parameters()]#\n",
    "\n",
    "x = [2.0,3.0,4.0]\n",
    "x = [Value(x,label=f\"X_{idx+1}\") for idx, x in enumerate(x)]\n",
    "neural_network = MLP(3,[4,4,1])\n",
    "# first layer has 4 neurons each with input size of 3\n",
    "# second layer has also 4 neurons each with input size of 4\n",
    "# third layer has 1 neuron with input size of 4\n",
    "\n",
    "res = neural_network(x)\n",
    "\n",
    "print(type(res))\n",
    "print(res)\n",
    "\n",
    "draw_dot(neural_network(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BACKPRPOPAGATION WITH GRADIENT DESCENT\n",
    "\n",
    "Okay, so now we have created a neural network. It is finally time to start optimizing it.     \n",
    "Optimize it for what? that is a good question. We have nothing to optimize to in this specific moment. \n",
    "\n",
    "1. To optimize it, we need to define **training data**. And a cost function.In our example this will be multiple pairs of x and y, where x is n valued vector and y is a scalar that we want to predict based on x.\n",
    "2. Next we will define cost function on top of neural network that summarizes how much nn forward pass output on top of x deviates from y. \n",
    "3. Then we will define gradients for all w-s in regards to that cost function and substract gradient * learning_rate from the original w values. \n",
    "    + In short, as gradient is just a collection of partial derivatives. As a whole it gives us the direction of function parameter movements in the steepest ascent. Opposite to that gives us the deepest descent. [Explanation](https://www.notion.so/dataleaper/CALCULUS-course-d74f76e4fead433a913704a584b57c79?pvs=4#be62339628e04a8fbf7cbe3dadc83174)\n",
    "    + Thus moving that will give us the deepest cost reduction\n",
    "\n",
    "\n",
    "For the cost function we do forward propagation with data multiple times, then we compare the result with real y and sum the differences up. \n",
    "\n",
    "We now try to move NN params in direction so that cost function would go down. To do this we call backward on cost function. \n",
    "\n",
    "If we have n training pairs, each parameter (w or b) has been used n times directly to get some calculation result. \n",
    "** Recall that each parameter is used only within 1 neuron as multiplicator for particular input. \n",
    "\n",
    "\n",
    "**NB** Important thing to take into account \n",
    "\n",
    "\n",
    "- With the recursive backward methodology we are using we will calculate first parent derivatives and then child derivatives. \n",
    "For child $$w_i$$, the derivative will be calculated only directly to $$\\frac{dw_i}{dh}$$ and then multiplied with the total dh derivative.\n",
    "It has only 1 parent thus only 1 backward call. However, $$w_i$$ will still have effect throuhgh multiple parents. \n",
    "- The end result cos function is based on somethin like a sum for all training pairs. Thus with each training example we are adding to the cost function which meand for each training example we will also to backpropgataion       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONE TRAINING EXAMPLE \n",
    "\n",
    "https://www.youtube.com/watch?v=KUw20-24YNE&t=124s&ab_channel=AndresNamm  -- video explaining backwards call amount. \n",
    "\n",
    "In 1 forward pass 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= [\n",
    "    [Value(2.0,label='X_11'),Value(3.0,label='X_12'),Value(4.0,label='X_13')],\n",
    "]\n",
    "\n",
    "y = [Value(1.0,label='Y')]\n",
    "\n",
    "neural_network = MLP(3,[4,4,1])\n",
    "\n",
    "\n",
    "y_pred = [neural_network(x_i) for x_i in x] #\n",
    "cost:Value = sum((y_pred_i-y_i)**2 for y_i, y_pred_i in zip(y,y_pred))# type: ignore #\n",
    "\n",
    "\n",
    "print(cost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost.backward()\n",
    "draw_dot(cost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTIPLE TRAINING SETS \n",
    "\n",
    "Here w.grad is updated from 2 different n-s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x= [\n",
    "    [2.0,3.0,4.0],\n",
    "    [1.0,2.0,3.0]\n",
    "]\n",
    "\n",
    "x = [[Value(xi,label=f'X{idx_i+1}{idxi+1}') for idxi,xi in enumerate(x_i)] for idx_i, x_i in enumerate(x)]\n",
    "\n",
    "\n",
    "y = [1.0,-1.0]\n",
    "y = [Value(yi,label=f'Y{idx+1}') for idx,yi in enumerate(y)]\n",
    "\n",
    "\n",
    "neural_network = MLP(3,[4,4,1])\n",
    "\n",
    "y_pred = [neural_network(x_i) for x_i in x] #\n",
    "cost:Value = sum((y_pred_i-y_i)**2 for y_i, y_pred_i in zip(y,y_pred))# type: ignore #\n",
    "\n",
    "cost.backward()\n",
    "draw_dot(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATHEMATICAL (KINDA) EXPLANATION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](data/img/training_example_derivative.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example the derivative for ${w_{11}}$ in regards to cost function is based on the derivative sum rule (among other things) where derivatives from $NN(x_{ij})-y_{ij}$ all sum up to get final derivative of $w_{ij}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENTING BACKPROPAGATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(nn: MLP, x, y, trainig_rate, iterations=100):\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # With each training pair we are going through, we will do 1 forward pass of the network\n",
    "        y_pred=[nn(xi) for xi in x]    \n",
    "\n",
    "        # We add up the total cost. With the current approach this total cost will be the weights multiplied with\n",
    "        # with all forward passes and compared with real training data. For each forward paas the cost function becomes this \n",
    "        # Giant calculation #\n",
    "        cost: Value =  sum([(y_i-y_pred_i)**2 for y_i,y_pred_i in zip(y_pred,y)]) # type: ignore\n",
    "\n",
    "        for param in nn.parameters():\n",
    "            param.grad=0.0\n",
    "\n",
    "        # Calculate gradient for neural network parameters w-s and b-s\n",
    "        cost.backward()\n",
    "\n",
    "        #print(\",\".join([f\"{param.label}={param.data}\" for param in all_params]))\n",
    "        for param in nn.parameters():\n",
    "            param.data -= trainig_rate*param.grad\n",
    "\n",
    "        print(cost)\n",
    "\n",
    "\n",
    "\n",
    "nn = MLP(3,[4,4,1])\n",
    "\n",
    "x= [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "\n",
    "x = [[Value(xi,label=f'X{idx_i+1}{idxi+1}') for idxi,xi in enumerate(x_i)] for idx_i, x_i in enumerate(x)]\n",
    "\n",
    "\n",
    "y = [1.0, -1.0, -1.0, 1.0]\n",
    "y = [Value(yi,label=f'Y{idx+1}') for idx,yi in enumerate(y)]\n",
    "\n",
    "# You can try different params here\n",
    "neural_network = MLP(3,[100,100,1]) \n",
    "\n",
    "\n",
    "backpropagation(neural_network,x,y,0.1,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neural_network(x[0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neural_network(x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADDITIONAL COMMENTS FOR TRAINING \n",
    "\n",
    "- With each training pair we are going through, we will do additional backpropagation with this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
